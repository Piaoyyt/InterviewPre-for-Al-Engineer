# 机器学习知识点总结


## 1.监督模型

### 1.1SVM
> SVM全名叫支持向量机，是一种十分经典的分类和回归模型，为什么叫支持向量，
> 是因为该模型的最终形式只和支持向量，即那些位于决策面上的向量有关,根据数据
> 的线性可分、线性不可分以及非线性关系，可以将对应的SVM分成硬间隔下的SVM，
> 软间隔下的SVM，以及结合核函数变换下的SVM。
#### 1.硬间隔下的SVM
> 所谓的硬间隔，是针对数据样本线性可分的情况，这个时候我们希望所有的样本都能够
> 正确分出来，并且得到的决策面要最优，怎么去定义这个最优？这个就是SVM里面比较
> 核心的部分，也就是目标函数部分。
1. 目标函数和约束条件
   > 假定最终得到的决策面是两个互相平行的直线y=wx+b（两个是因为一边对应一类，中间的
   > 部分就是类间的差异），我们所要得到的当前是这两个平行直线最大间隔的情况，
   > 对于两类的情况，我们可以简化成一个标签为1，另一个为-1，并且我们希望模型实际
   > 预测的结果也和标签尽可能一致（即两类样本下预测的结果准确而且差异尽可能大），结果
   > 准确对应的方程是y * (wx+b) >= 1，即每一个样本都预测正确，而差异尽可能大
   > 则对应两个分类面的间隔尽可能大，即1/(w)**2尽可能的大，也就是对应w的L2范数的平方
   > 尽可能的小，所以目标函数可以表示为最小化w的L2范数平方，同时带有一个约束条件
   > 各个样本的标签和实际预测的结果乘积要大于1。
2. 求解过程
   > 此时问题转化为了一个在一定约束条件下求某一个目标函数的最优值，对于这类问题，我们常常
   > 通过拉格朗日算子将约束和待求解目标合并在一起，这样就会引入拉格朗日算子alpha，并且
   > 此时问题变成了最小最大化问题（最大化约束，最小化目标），我们的目标是求最优的参数w和b，如果直接对这个最小最大化
   > 问题去求解比较复杂（因为通常求最小化问题直接求偏导即可，凸优化问题），所以这里利用到了
   > 对偶思想（最大化最小的最优解比最小化最大的最优解要小），转化成了最大化最小的问题，此时最小化
   > 过程可以把拉格朗日乘子alpha当成一个固定的值，然后对w和b求偏导，得到各自的表达式，然后带入进行
   > 相当于就是求alpha的最大化，对式子取负，即转换成求最小化的问题，通过SMO算法去求解即可，
   > 即每一次求两个alpha参数，固定住其它的参数。
   - KKT条件
     >  什么是kkt条件，即当我们在求解一个带约束问题的待优化问题的时候，通常我们会将其整合到一个方程式
     里面，这时候对这个方方程式求最小最大化的最优值，满足几个条件，即整个方程关于待优化的参数求导为0，
     灯饰约束都为0，不等式约束（均为大于等于的）和为0（带上拉格朗日乘子）。
   - SMO算法
     > 最终的优化是要对n个alpha进行计算，同时对n个去算比较复杂，smo的想法是每一次取两个alpha出来，固定住
     其它的alpha。
#### 2.软间隔下的SVM
   > 在硬间隔里面我们是假设所有的样本都能够正确的分类，但是实际里面很难做到如此，很可能存在
> 个别的难分类样本，所谓的难样本即不在最大分类间隔两侧的，即处在分类间隔线内部的样本，对于这样的样本我们所要求 的 
   间隔没有那么苛刻，也即软间隔的概念。
1. 目标函数和约束条件
   > 软间隔本质上体现在约束条件的式子里面，y * (w * x + b) < 1了，即此时的约束条件相比于硬间隔下要放松了，至于放松了多少，这里是给每一个样本设置一个间隔
   参数，即y * (w * x + b) >= 1 - gap，gap越大，说明样本越难分，当gap为0时，和硬间隔就没有区别了；同时我们需要对gap也进行一个不小于0的
   约束。改变了约束条件当然我们不希望这个gap无限制的变化，而是希望gap尽可能的小，所有目标函数里面多了一个对gap的惩罚项，gap前面的参数
   我们称之为惩罚因子C，惩罚因子项可以理解为经验风险（即根据观测数据来得到的模型尽可能使得这些样本的误差尽可能的小，当C越大时，
   模型对训练数据的拟合能力得到加强，太大会过拟合），而前一项对w参数的L2范数平方和可以理解为是结构风险（在分类正确即误差一定的情况下，
   让模型的泛化性更好，这里其实就可以看到模型参数越复杂，越不利于模型的泛化性提升）。
2. 求解过程
   >  求解过程和硬间隔基本一样，对于待优化的目标和约束条件整合到一起，然后最小最大变成最大最小，然后通过对偶问题进行最优值的求解，结合了KKT条件和SMO算法
   得到最优的alpha值，然后就可以表示出w和b了。
#### 3.非线性变化
> 对于非线性即线性不可分的数据样本，我们这时候直接用SVM去求解是不行的，这时候可以尝试对数据样本进行升维处理，即原本的低维已经不足以
> 来区分样本了，这时候就要考虑结合新的特征属性，可以理解成创造新的特征值，本质上一个维度的变化，但是维度一变，就会导致计算的过程变得
> 复杂，有没有一种方式可以使得计算比较简便，这时候核函数就出来了，核函数的本质是不需要计算数据样本在高维上的内积计算过程，而是直接通过
> 低维的映射直接得到高维内积同样的结果，这样做就避免了高维计算，这样做的理论支撑还是SVM最后的最优模型参数由各个数据样本的alpha决定的，
> 而alpha又是由数据样本之间的内积决定的，所以可以通过这样去得到最终的结果
- 常见的核函数
  ![Kernels](pics/SVMKernels.png)
      1. 线性核函数：即低维（x1,x2)→x1 * x2  
      2. 高斯核函数: 即低维(x1, x2)→代入高斯形式的函数（x1和x2之间的差平方以及高斯的mu和sigma）  
      3. 拉普拉斯。
      4. sigmoid激活核函数。
### 1.2感知机算法（二分类算法）

### 1.3逻辑回归算法（二分类算法）

### 1.4线性回归算法

### 1.5贝叶斯（）

### 1.6决策树
