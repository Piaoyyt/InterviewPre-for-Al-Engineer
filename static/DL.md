
# Deep Learning Part
> This is the DL part which aims to summarize the knowledge of the 
> deep learning for Algorithm Interview.

## Main Content
- [基本概念](#BasicC)   
   - [优化器](#Optim)
- [深度网络模型](#NetModel)
  

## <a id="BasicC"></a>1.Basic Concpet of the DL

### <a id="Optim"></a>1.1.优化器
> **概念**：深度网络模型训练里面负责来控制网络参数更新方向和更新速度的结构。
#### 1.SGD
SGD的全称为Stochastic Gradient Descent,即随机梯度下降，因为SGD里面参数更新的
方向只和当前计算得到的梯度有关，并且参数的更新速率也是一个固定值，属于最早的一种
优化器。
#### 2.SGDM
相比于SGD增加了动量的概念，所谓的动量即考虑了前面的下降方向和下降幅度，让当前时刻的更新
不单单由当前的梯度决定，还受前面的更新方向影响，即动量的体现。
#### 3.Adagrad
自适应的梯度，这里所谓的自适应其实指的是梯度更新里面的学习率自适应，即梯度更新的快慢
程度，加入了不同时刻的梯度平方信息作为调整学习率的依据，使得学习率能够不断变化，这里
是不断的变小。
#### 4.RMSprop
不同于Adagrad里面将梯度平方和的信息作为调整学习率的依据，这样会导致二阶动量会不断的积累，
导致学习率下降的速度过快；这里是采用指数平均公式来计算，避免积累的问题。
#### 5.Adam
在梯度下降方向和梯度下降幅度两方面都加入了指数平均的思想，使得当前参数更新的方向更加的稳定，
并且为了防止在第一次更新的时候梯度更新会偏向于0，在里面还加入了偏置校正。
### <a id="Convolution"></a>1.2.卷积

## <a id="NetModel"></a>2.网络模型
#### 1.LeNet
> LeNet是最早被用来设计进行手写数字识别的网络，由几层卷积层和全连接层组成，结构很简单。
#### 2.AlexNet
> AlexNet是当年用于ImageNet目标检测竞赛的冠军网络，核心点是用了大的卷积核(5、7)，采用了局部响
应归一化（LRN)， 并加入了dropout。
#### 3.VggNet
> 相比于之前的网络结构，核心点在于探索了较深的网络可以提高网络的性能，用小的卷积核代替之前大的卷积核，
并且增加了网络的深度，增加了网络的非线性表达能力。因为两个大小为3的卷积核相当于一个大小为5的卷积核。
#### 4.GoogleNet
> 核心在于Inception模块的设计，该模块通过多个小的卷积核对feature map进行不同的处理，最
后将不同通道数的输出feature map拼接起来，增加了网络的宽度，实际上增加了网络的学习能力， 即每一层
能够学习到的东西更加丰富【1*1的卷积核实现维度的升降维】。 
#### 5.Incepv2-Incepv4
> Inception的不同版本都可以看作是基于googleNet做的改进。
  > - v2：主要加入了BN到网络里面，即批量归一化，对于同一通道的不同样本特征图进行归一化操作，原始论文里面
说降低了训练时的协方差偏移，使得训练能够更快的收敛。
  > - v3: 主要是将普通的卷积进行空间上分解，分解成宽和高度的两步卷积，但是尽管这种可以降低参数量，但是
这两种卷积方式并不能等价，所以如果对于网络性能要求比较好的时候这种方式通常不会采用。
  > - v4: 主要是结合了ResNet里面的残差结构来设计模型。
#### 6.ResNet
  >核心点就是残差块的设计，所谓的残差块其实就相当于一个恒等映射，一般的网络层是将输入直接映射到输出，当网络
  > 较深的时候，如果网络能力下降会使得输出丢失掉很多有用的信息，而残差块的设计直接让输入直接和输出相连，网络
  > 层相当于只学习到了输出和输入之间的差，即使是最差的情况，网络层的输出不会丢失输入的有效信息，这样的设计能
  > 够使得当网络层很深的时候仍然能够学习到有用的东西。
#### 7.SeNet
> SeNet全称为Squeeze and Excitation Net，即先将宽和高压缩成一维的形状，那么每一个通道就得到了各自的值，
> 这个值作为激活前的值经过sigmoid能够得到一个权重，即每个通道的权重，可以看作通道上的注意力，然后和原始的特征图
> 进行相乘。
#### 8.MobileNet
> MobileNet是轻量化网络里面的一个典型代表，轻量化的方式是对于卷积操作进行设计的，即不同于常规的卷积，而是采用
> 分组卷积的形似，将原始特征图的通道进行分组，使得卷积的参数量能够大大降低；同时为了避免分组卷积使得不同通道之间
> 的联系被忽视，在后面又加入了逐像素的卷积操作，即大小为1的卷积核，这种可以融合不同通道之间的信息。
#### 9.ShuffleNet
> 核心点在于shuffle the channel，我们知道分组卷积会造成一个后果，就是不同组的channel之间的信息被忽略了，因
> 为组分好之后，不同组就无法交互了，而shuffle就是提前把通道进行打乱，然后对打乱后的分组，这样就可以解决这个问题。